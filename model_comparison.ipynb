{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind DT — Model Comparison Notebook\n",
    "\n",
    "This notebook loads one or more CSV files produced by the pipeline's sinks (e.g., `preds_lgbm.csv`, `preds_svr.csv`), computes comparable metrics, and makes a few diagnostic plots.\n",
    "\n",
    "**What you need:**\n",
    "- CSVs with columns at least: `ts, y, y_hat` (and optionally `model, v, pi`).\n",
    "- Put them in a folder and point `DATA_DIR` below.\n",
    "- Set `RATED_POWER_KW` to your turbine's rated power for normalized errors.\n",
    "\n",
    "> Charts use **matplotlib** (no seaborn), one chart per figure, and avoid explicit colors to match your environment rules."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Configuration ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the directory containing your CSV outputs (change as needed)\n",
    "DATA_DIR = Path(\"out\")  # e.g., Path(r\"C:/Users/.../wind-dt/out\")\n",
    "\n",
    "# Glob pattern for CSVs to include. You can list specific files instead.\n",
    "CSV_GLOB = \"*.csv\"  # e.g., \"preds_*.csv\"\n",
    "\n",
    "# Turbine rated power (kW) for normalized error. Adjust to your turbine.\n",
    "RATED_POWER_KW = 2050.0\n",
    "\n",
    "# Optional: choose a time window for plots (None means use full range)\n",
    "TIME_START = None  # e.g., \"2020-06-01\"\n",
    "TIME_END   = None  # e.g., \"2020-06-15\"\n",
    "\n",
    "# Downsample factor for plotting (plot every Nth point to keep figures light)\n",
    "PLOT_EVERY = 10"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Load CSVs ---\n",
    "import glob\n",
    "\n",
    "files = sorted([Path(p) for p in glob.glob(str(DATA_DIR / CSV_GLOB))])\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No CSV files match pattern {CSV_GLOB} in {DATA_DIR.resolve()}\")\n",
    "\n",
    "dfs = []\n",
    "for p in files:\n",
    "    df = pd.read_csv(p)\n",
    "    if \"ts\" not in df.columns or \"y\" not in df.columns or \"y_hat\" not in df.columns:\n",
    "        raise ValueError(f\"{p.name} must contain at least columns: ts, y, y_hat. Has: {list(df.columns)[:10]}\")\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], utc=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"ts\"])\n",
    "    if \"model\" not in df.columns or df[\"model\"].isna().all():\n",
    "        stem = p.stem\n",
    "        inferred = stem.replace(\"preds_\", \"\")\n",
    "        df[\"model\"] = inferred\n",
    "    for col in [\"y\",\"y_hat\",\"v\",\"pi\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    if TIME_START is not None:\n",
    "        df = df[df[\"ts\"] >= pd.Timestamp(TIME_START, tz=\"UTC\")]\n",
    "    if TIME_END is not None:\n",
    "        df = df[df[\"ts\"] <= pd.Timestamp(TIME_END, tz=\"UTC\")]\n",
    "    dfs.append(df)\n",
    "\n",
    "len(files), [f.name for f in files]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Helper: infer sampling interval in hours per model ---\n",
    "def infer_delta_hours(ts_series: pd.Series) -> float:\n",
    "    if ts_series.size < 2:\n",
    "        return np.nan\n",
    "    dt = ts_series.sort_values().diff().dropna().median()\n",
    "    return float(dt.total_seconds()) / 3600.0\n",
    "\n",
    "sampling_by_model = {}\n",
    "for df in dfs:\n",
    "    model = df[\"model\"].iloc[0]\n",
    "    sampling_by_model[model] = infer_delta_hours(df[\"ts\"])\n",
    "sampling_by_model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Compute per-model metrics ---\n",
    "def safe_mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = y_true > 1e-6\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)\n",
    "\n",
    "rows = []\n",
    "for df in dfs:\n",
    "    model = df[\"model\"].iloc[0]\n",
    "    d = df.dropna(subset=[\"y\",\"y_hat\"]).copy()\n",
    "    if d.empty:\n",
    "        continue\n",
    "    err = d[\"y_hat\"] - d[\"y\"]\n",
    "    mae = float(np.mean(np.abs(err)))\n",
    "    rmse = float(np.sqrt(np.mean(err**2)))\n",
    "    nmae = mae / RATED_POWER_KW * 100.0\n",
    "    dh = sampling_by_model.get(model, np.nan)\n",
    "    energy_bias_kwh = float(np.nansum(err * (dh if np.isfinite(dh) else 0.0)))\n",
    "    pi_mean = float(d[\"pi\"].mean()) if \"pi\" in d.columns else np.nan\n",
    "    pi_median = float(d[\"pi\"].median()) if \"pi\" in d.columns else np.nan\n",
    "    pi_p10 = float(d[\"pi\"].quantile(0.1)) if \"pi\" in d.columns else np.nan\n",
    "    pi_p90 = float(d[\"pi\"].quantile(0.9)) if \"pi\" in d.columns else np.nan\n",
    "    rows.append({\n",
    "        \"model\": model,\n",
    "        \"rows\": int(len(d)),\n",
    "        \"sampling_h\": float(dh) if np.isfinite(dh) else np.nan,\n",
    "        \"MAE_kW\": mae,\n",
    "        \"RMSE_kW\": rmse,\n",
    "        \"NMAE_%_of_rated\": nmae,\n",
    "        \"MAPE_%\": safe_mape(d[\"y\"], d[\"y_hat\"]),\n",
    "        \"EnergyBias_kWh\": energy_bias_kwh,\n",
    "        \"PI_mean\": pi_mean,\n",
    "        \"PI_median\": pi_median,\n",
    "        \"PI_p10\": pi_p10,\n",
    "        \"PI_p90\": pi_p90\n",
    "    })\n",
    "\n",
    "metrics = pd.DataFrame(rows).sort_values([\"NMAE_%_of_rated\",\"RMSE_kW\"])\n",
    "metrics"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save metrics table\n",
    "metrics_path = DATA_DIR / \"model_metrics.csv\"\n",
    "metrics.to_csv(metrics_path, index=False)\n",
    "print(f\"Saved metrics to: {metrics_path.resolve()}\")\n",
    "metrics"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Merge multiple models for head-to-head plots ---\n",
    "from functools import reduce\n",
    "\n",
    "def prepare_for_merge(df):\n",
    "    m = df.copy()\n",
    "    mdl = m[\"model\"].iloc[0]\n",
    "    m = m[[\"ts\",\"y\",\"y_hat\"]].rename(columns={\"y_hat\": f\"y_hat_{mdl}\"})\n",
    "    return m\n",
    "\n",
    "merged = reduce(lambda left,right: pd.merge(left, right, on=[\"ts\",\"y\"], how=\"outer\"),\n",
    "                [prepare_for_merge(df) for df in dfs])\n",
    "\n",
    "merged = merged.sort_values(\"ts\")\n",
    "merged.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Plot: actual vs predicted (subset, top 1–3 models) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_cols = [c for c in merged.columns if c.startswith(\"y_hat_\")][:3]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "if pred_cols:\n",
    "    s = merged if PLOT_EVERY <= 1 else merged.iloc[::PLOT_EVERY, :]\n",
    "    plt.plot(s[\"ts\"], s[\"y\"], label=\"Actual (y)\")\n",
    "    for c in pred_cols:\n",
    "        plt.plot(s[\"ts\"], s[c], label=c)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Power (kW)\")\n",
    "    plt.title(\"Actual vs Predicted (subset)\")\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"No prediction columns found\", ha=\"center\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Scatter plots: y_hat vs y per model ---\n",
    "for df in dfs:\n",
    "    mdl = df[\"model\"].iloc[0]\n",
    "    d = df.dropna(subset=[\"y\",\"y_hat\"])\n",
    "    if d.empty:\n",
    "        continue\n",
    "    fig = plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.scatter(d[\"y\"], d[\"y_hat\"], s=5)\n",
    "    plt.xlabel(\"Actual y (kW)\")\n",
    "    plt.ylabel(\"Predicted y_hat (kW)\")\n",
    "    plt.title(f\"y_hat vs y — {mdl}\")\n",
    "    plt.plot([d[\"y\"].min(), d[\"y\"].max()], [d[\"y\"].min(), d[\"y\"].max()])\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Residual histograms per model ---\n",
    "for df in dfs:\n",
    "    mdl = df[\"model\"].iloc[0]\n",
    "    d = df.dropna(subset=[\"y\",\"y_hat\"])\n",
    "    if d.empty:\n",
    "        continue\n",
    "    err = (d[\"y_hat\"] - d[\"y\"]).values\n",
    "    fig = plt.figure(figsize=(5, 3.5))\n",
    "    plt.hist(err, bins=50)\n",
    "    plt.xlabel(\"Residual (y_hat - y) kW\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Residuals — {mdl}\")\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
