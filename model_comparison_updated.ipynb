{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adb2733",
   "metadata": {},
   "source": [
    "# Wind DT — Model Comparison Notebook\n",
    "\n",
    "This notebook loads one or more CSV files produced by the pipeline's sinks (e.g., `preds_lgbm.csv`, `preds_svr.csv`), computes comparable metrics, and makes a few diagnostic plots.\n",
    "\n",
    "**What you need:**\n",
    "- CSVs with columns at least: `ts, y, y_hat` (and optionally `model, v, pi`).\n",
    "- Put them in a folder and point `DATA_DIR` below.\n",
    "- Set `RATED_POWER_KW` to your turbine's rated power for normalized errors.\n",
    "\n",
    "> Charts use **matplotlib** (no seaborn), one chart per figure, and avoid explicit colors to match your environment rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001a29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to the directory containing your CSV outputs (change as needed)\n",
    "DATA_DIR = Path(\"out\")  # e.g., Path(r\"C:/Users/.../wind-dt/out\")\n",
    "\n",
    "# Glob pattern for CSVs to include. You can list specific files instead.\n",
    "CSV_GLOB = \"*.csv\"  # e.g., \"preds_*.csv\"\n",
    "\n",
    "# Turbine rated power (kW) for normalized error. Adjust to your turbine.\n",
    "RATED_POWER_KW = 2050.0\n",
    "\n",
    "# Optional: choose a time window for plots (None means use full range)\n",
    "TIME_START = None  # e.g., \"2020-06-01\"\n",
    "TIME_END   = None  # e.g., \"2020-06-15\"\n",
    "\n",
    "# Downsample factor for plotting (plot every Nth point to keep figures light)\n",
    "PLOT_EVERY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a918b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, ['preds_lag.csv'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load CSVs ---\n",
    "import glob\n",
    "\n",
    "files = sorted([Path(p) for p in glob.glob(str(DATA_DIR / CSV_GLOB))])\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No CSV files match pattern {CSV_GLOB} in {DATA_DIR.resolve()}\")\n",
    "\n",
    "dfs = []\n",
    "for p in files:\n",
    "    df = pd.read_csv(p)\n",
    "    #if \"ts\" not in df.columns or \"y\" not in df.columns or \"y_hat\" not in df.columns:\n",
    "        #raise ValueError(f\"{p.name} must contain at least columns: ts, y, y_hat. Has: {list(df.columns)[:10]}\")\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], utc=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"ts\"])\n",
    "    if \"model\" not in df.columns or df[\"model\"].isna().all():\n",
    "        stem = p.stem\n",
    "        inferred = stem.replace(\"preds_\", \"\")\n",
    "        df[\"model\"] = inferred\n",
    "    for col in [\"y\",\"y_hat_H1\",\"y_hat_D1\",\"y_hat_W1\",\"y_hat_M1\",\"v\",\"pi\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    if TIME_START is not None:\n",
    "        df = df[df[\"ts\"] >= pd.Timestamp(TIME_START, tz=\"UTC\")]\n",
    "    if TIME_END is not None:\n",
    "        df = df[df[\"ts\"] <= pd.Timestamp(TIME_END, tz=\"UTC\")]\n",
    "    dfs.append(df)\n",
    "\n",
    "len(files), [f.name for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2469ae2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lag_adapter': 0.16666666666666666}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Helper: infer sampling interval in hours per model ---\n",
    "def infer_delta_hours(ts_series: pd.Series) -> float:\n",
    "    if ts_series.size < 2:\n",
    "        return np.nan\n",
    "    dt = ts_series.sort_values().diff().dropna().median()\n",
    "    return float(dt.total_seconds()) / 3600.0\n",
    "\n",
    "sampling_by_model = {}\n",
    "for df in dfs:\n",
    "    model = df[\"model\"].iloc[0]\n",
    "    sampling_by_model[model] = infer_delta_hours(df[\"ts\"])\n",
    "sampling_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c179a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['y_hat']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_31624\\1239544633.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m rows = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;28;01min\u001b[39;00m dfs:\n\u001b[32m     12\u001b[39m     model = df[\u001b[33m\"model\"\u001b[39m].iloc[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     d = df.dropna(subset=[\u001b[33m\"y\"\u001b[39m,\u001b[33m\"y_hat\"\u001b[39m]).copy()\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m d.empty:\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     16\u001b[39m     err = d[\u001b[33m\"y_hat\"\u001b[39m] - d[\u001b[33m\"y\"\u001b[39m]\n",
      "\u001b[32mc:\\Users\\gebruiker\\Desktop\\Digital Twin Engineering\\wind-dt-forecast\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6673\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6674\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6675\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6676\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6677\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6678\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6679\u001b[39m \n\u001b[32m   6680\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['y_hat']"
     ]
    }
   ],
   "source": [
    "# --- Compute per-model metrics ---\n",
    "def safe_mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    mask = np.abs(y_true) > 1e-6\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)\n",
    "\n",
    "rows = []\n",
    "# horizon columns we may encounter (including legacy single 'y_hat')\n",
    "h_cols = [\"y_hat\", \"y_hat_H1\", \"y_hat_D1\", \"y_hat_W1\", \"y_hat_M1\"]\n",
    "for df in dfs:\n",
    "    model = df[\"model\"].iloc[0]\n",
    "    # find which horizon/prediction columns exist in this dataframe\n",
    "    present_h = [c for c in h_cols if c in df.columns]\n",
    "    if not present_h:\n",
    "        # nothing to evaluate for this model\n",
    "        continue\n",
    "    for h in present_h:\n",
    "        d = df.dropna(subset=[\"y\", h]).copy()\n",
    "        if d.empty:\n",
    "            continue\n",
    "        err = d[h] - d[\"y\"]\n",
    "        mae = float(np.mean(np.abs(err)))\n",
    "        rmse = float(np.sqrt(np.mean(err**2)))\n",
    "        nmae = mae / RATED_POWER_KW * 100.0\n",
    "        dh = sampling_by_model.get(model, np.nan)\n",
    "        energy_bias_kwh = float(np.nansum(err * (dh if np.isfinite(dh) else 0.0)))\n",
    "        pi_mean = float(d[\"pi\"].mean()) if \"pi\" in d.columns else np.nan\n",
    "        pi_median = float(d[\"pi\"].median()) if \"pi\" in d.columns else np.nan\n",
    "        pi_p10 = float(d[\"pi\"].quantile(0.1)) if \"pi\" in d.columns else np.nan\n",
    "        pi_p90 = float(d[\"pi\"].quantile(0.9)) if \"pi\" in d.columns else np.nan\n",
    "        rows.append({\n",
    "            \"model\": model,\n",
    "            \"horizon\": (h.replace(\"y_hat_\", \"\") if h.startswith(\"y_hat_\") else (\"H1\" if h == \"y_hat\" else h)),\n",
    "            \"rows\": int(len(d)),\n",
    "            \"sampling_h\": float(dh) if np.isfinite(dh) else np.nan,\n",
    "            \"MAE_kW\": mae,\n",
    "            \"RMSE_kW\": rmse,\n",
    "            \"NMAE_%_of_rated\": nmae,\n",
    "            \"MAPE_%\": safe_mape(d[\"y\"], d[h]),\n",
    "            \"EnergyBias_kWh\": energy_bias_kwh,\n",
    "            \"PI_mean\": pi_mean,\n",
    "            \"PI_median\": pi_median,\n",
    "            \"PI_p10\": pi_p10,\n",
    "            \"PI_p90\": pi_p90\n",
    "        })\n",
    "\n",
    "metrics = pd.DataFrame(rows)\n",
    "if not metrics.empty:\n",
    "    metrics = metrics.sort_values([\"NMAE_%_of_rated\",\"RMSE_KW\"]) if \"RMSE_KW\" in metrics.columns else metrics.sort_values([\"NMAE_%_of_rated\",\"RMSE_kW\"])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics table\n",
    "metrics_path = DATA_DIR / \"model_metrics.csv\"\n",
    "metrics.to_csv(metrics_path, index=False)\n",
    "print(f\"Saved metrics to: {metrics_path.resolve()}\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge multiple models for head-to-head plots ---\n",
    "from functools import reduce\n",
    "\n",
    "def prepare_for_merge(df):\n",
    "    m = df.copy()\n",
    "    mdl = m[\"model\"].iloc[0]\n",
    "    m = m[[\"ts\",\"y\",\"y_hat\"]].rename(columns={\"y_hat\": f\"y_hat_{mdl}\"})\n",
    "    return m\n",
    "\n",
    "merged = reduce(lambda left,right: pd.merge(left, right, on=[\"ts\",\"y\"], how=\"outer\"),\n",
    "                [prepare_for_merge(df) for df in dfs])\n",
    "\n",
    "merged = merged.sort_values(\"ts\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot: actual vs predicted (subset, top 1–3 models) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_cols = [c for c in merged.columns if c.startswith(\"y_hat_\")][:3]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "if pred_cols:\n",
    "    s = merged if PLOT_EVERY <= 1 else merged.iloc[::PLOT_EVERY, :]\n",
    "    plt.plot(s[\"ts\"], s[\"y\"], label=\"Actual (y)\")\n",
    "    for c in pred_cols:\n",
    "        plt.plot(s[\"ts\"], s[c], label=c)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Power (kW)\")\n",
    "    plt.title(\"Actual vs Predicted (subset)\")\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"No prediction columns found\", ha=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f14404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scatter plots: y_hat vs y per model ---\n",
    "for df in dfs:\n",
    "    mdl = df[\"model\"].iloc[0]\n",
    "    d = df.dropna(subset=[\"y\",\"y_hat\"])\n",
    "    if d.empty:\n",
    "        continue\n",
    "    fig = plt.figure(figsize=(4.5, 4.5))\n",
    "    plt.scatter(d[\"y\"], d[\"y_hat\"], s=5)\n",
    "    plt.xlabel(\"Actual y (kW)\")\n",
    "    plt.ylabel(\"Predicted y_hat (kW)\")\n",
    "    plt.title(f\"y_hat vs y — {mdl}\")\n",
    "    plt.plot([d[\"y\"].min(), d[\"y\"].max()], [d[\"y\"].min(), d[\"y\"].max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Residual histograms per model ---\n",
    "for df in dfs:\n",
    "    mdl = df[\"model\"].iloc[0]\n",
    "    d = df.dropna(subset=[\"y\",\"y_hat\"])\n",
    "    if d.empty:\n",
    "        continue\n",
    "    err = (d[\"y_hat\"] - d[\"y\"]).values\n",
    "    fig = plt.figure(figsize=(5, 3.5))\n",
    "    plt.hist(err, bins=50)\n",
    "    plt.xlabel(\"Residual (y_hat - y) kW\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Residuals — {mdl}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b9559",
   "metadata": {},
   "source": [
    "# ⏱️ Multi‑Horizon Forecast Comparison (H1, D1, W1, M1)\n",
    "\n",
    "*Appended automatically on 2025-10-08 19:44 UTC.*\n",
    "\n",
    "This section compares **four different time‑horizon predictions** found in the CSV:\n",
    "\n",
    "- `y_hat_H1` — short horizon (e.g., next step / hour)\n",
    "- `y_hat_D1` — daily horizon\n",
    "- `y_hat_W1` — weekly horizon\n",
    "- `y_hat_M1` — monthly horizon\n",
    "\n",
    "It computes common error metrics per **model × horizon** and visualizes them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Configuration ====\n",
    "# Set the path to your CSV file here. If left as None, a small in-memory demo is used.\n",
    "csv_path = None  # e.g., '/mnt/data/your_predictions.csv'\n",
    "\n",
    "# Columns expected in the CSV:\n",
    "# ts, model, v, y, y_hat_H1, y_hat_D1, y_hat_W1, y_hat_M1, pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Imports ====\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89bd9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Load Data ====\n",
    "from io import StringIO\n",
    "\n",
    "if csv_path is None:\n",
    "    demo_csv = StringIO(\"\"\"ts,model,v,y,y_hat_H1,y_hat_D1,y_hat_W1,y_hat_M1,pi\n",
    "2020-02-14T12:00:00,lag_adapter,8.41154804229737,1312.77902832031,956.1858763868765,877.478747242494,1343.364147246193,473.18349406401177,\n",
    "2020-02-14T12:10:00,lag_adapter,8.94811565876007,1563.40854492188,1211.7134553682765,661.0247264533829,1174.8725162788635,632.8199656171491,\n",
    "\"\"\")\n",
    "    df = pd.read_csv(demo_csv)\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "# Parse timestamps if present\n",
    "if \"ts\" in df.columns:\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "\n",
    "# Basic sanity check\n",
    "required_cols = {\"model\",\"y\",\"y_hat_H1\",\"y_hat_D1\",\"y_hat_W1\",\"y_hat_M1\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in CSV: {missing}\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Compute Metrics Per Model × Horizon ====\n",
    "\n",
    "# Melt horizons into long format\n",
    "h_cols = [\"y_hat_H1\", \"y_hat_D1\", \"y_hat_W1\", \"y_hat_M1\"]\n",
    "long = df.melt(\n",
    "    id_vars=[c for c in df.columns if c not in h_cols],\n",
    "    value_vars=h_cols,\n",
    "    var_name=\"horizon\",\n",
    "    value_name=\"y_hat\",\n",
    ")\n",
    "\n",
    "# Normalize horizon labels like H1, D1, W1, M1\n",
    "long[\"horizon\"] = long[\"horizon\"].str.replace(\"y_hat_\", \"\", regex=False)\n",
    "\n",
    "def safe_mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    eps = 1e-9\n",
    "    denom = np.where(np.abs(y_true) < eps, np.nan, np.abs(y_true))\n",
    "    return np.nanmean(np.abs((y_true - y_pred) / denom)) * 100.0\n",
    "\n",
    "def compute_metrics(group: pd.DataFrame) -> Dict[str, float]:\n",
    "    y = group[\"y\"].astype(float).to_numpy()\n",
    "    yhat = group[\"y_hat\"].astype(float).to_numpy()\n",
    "    resid = y - yhat\n",
    "    mae = float(np.mean(np.abs(resid)))\n",
    "    rmse = float(np.sqrt(np.mean(resid**2)))\n",
    "    mape = float(safe_mape(y, yhat))\n",
    "    # R^2\n",
    "    ss_res = float(np.sum(resid**2))\n",
    "    ss_tot = float(np.sum((y - np.mean(y))**2)) if len(y) > 1 else np.nan\n",
    "    r2 = 1.0 - ss_res/ss_tot if ss_tot and not math.isclose(ss_tot, 0.0) else np.nan\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE_%\": mape, \"R2\": r2}\n",
    "\n",
    "metrics = (\n",
    "    long\n",
    "    .dropna(subset=[\"y\",\"y_hat\"])\n",
    "    .groupby([\"model\",\"horizon\"], as_index=False)\n",
    "    .apply(lambda g: pd.Series(compute_metrics(g)))\n",
    "    .sort_values([\"model\",\"horizon\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Display Metrics Table ====\n",
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "display_dataframe_to_user(\"Multi-Horizon Metrics by Model\", metrics)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2464b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Plot RMSE by Horizon (Grouped by Model) ====\n",
    "# One figure: x=horizon, grouped bars by model\n",
    "\n",
    "pivot_rmse = metrics.pivot(index=\"horizon\", columns=\"model\", values=\"RMSE\")\n",
    "ax = pivot_rmse.plot(kind=\"bar\", figsize=(8,4))\n",
    "ax.set_title(\"RMSE by Horizon and Model\")\n",
    "ax.set_xlabel(\"Horizon\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca176e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Example Time Series Plots: Actual vs Predicted (Per Horizon) ====\n",
    "# This produces four separate plots (one per horizon)\n",
    "# If you have many points, consider slicing with a date range or tail().\n",
    "\n",
    "for hz in [\"H1\",\"D1\",\"W1\",\"M1\"]:\n",
    "    col = f\"y_hat_{hz}\"\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    # Use one model at a time if multiple exist (or plot all predictions together if desired).\n",
    "    # Here, we'll plot the first model's series to keep the plot uncluttered.\n",
    "    first_model = df[\"model\"].iloc[0]\n",
    "    sub = df[df[\"model\"] == first_model].copy()\n",
    "\n",
    "    # If timestamps exist, sort by ts for a proper line chart\n",
    "    if \"ts\" in sub.columns:\n",
    "        sub = sub.sort_values(\"ts\")\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    if \"ts\" in sub.columns and pd.api.types.is_datetime64_any_dtype(sub[\"ts\"]):\n",
    "        x = sub[\"ts\"]\n",
    "    else:\n",
    "        x = np.arange(len(sub))\n",
    "\n",
    "    plt.plot(x, sub[\"y\"], label=\"y (actual)\")\n",
    "    plt.plot(x, sub[col], label=f\"{col} (pred)\")\n",
    "    plt.title(f\"Actual vs Predicted — {first_model} — {hz}\")\n",
    "    plt.xlabel(\"Time\" if \"ts\" in sub.columns else \"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Residual Distribution by Horizon (All Models Combined) ====\n",
    "import numpy as np\n",
    "\n",
    "for hz in [\"H1\",\"D1\",\"W1\",\"M1\"]:\n",
    "    hz_long = long[long[\"horizon\"] == hz].dropna(subset=[\"y\",\"y_hat\"])\n",
    "    if hz_long.empty:\n",
    "        continue\n",
    "    resid = (hz_long[\"y\"] - hz_long[\"y_hat\"]).astype(float).to_numpy()\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(resid, bins=30)\n",
    "    plt.title(f\"Residuals Histogram — {hz}\")\n",
    "    plt.xlabel(\"Residual (y - y_hat)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f7321",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- **MAPE** excludes points where `|y| < 1e-9` to avoid division-by-zero.\n",
    "- If you want **per-horizon leaderboards**, sort `metrics` within each horizon by RMSE or MAE.\n",
    "- To focus on a specific **date window**, filter `df` by `ts` before melting.\n",
    "- If you have **prediction intervals** (`pi`), you can extend this section with coverage and sharpness metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
